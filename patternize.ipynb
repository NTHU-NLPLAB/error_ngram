{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_data = '''\n",
    "# {+I　am//MW+} [-Fine//C-]{+fine//C+} , thank you !\n",
    "# Garlic and Echinacea tea : drink it when {+you//PS+} have infection , 　 it is simple but {+an//AR+} excellent antibiotic .\n",
    "# Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains , itis {+a//AR+} fast and effective way to relieve aches and pains .\n",
    "# [-everyone//C-]{+Everyone//C+} may use it in {+his　or　her//PS+} daily life .\n",
    "# Hi , my name is yanjun .I favorite day is sunday .\n",
    "# on sunday I get up at 8 o'clock in the morning I have a shower and brush my teeth , I have breakfast at 8:30 o'clock in a restaurant then I go running and read book .\n",
    "# I have lunch at 11:30 o'clock in the afternoon .I go to libary and learn english .\n",
    "# I have dinner at 6 o'clock in the evening .I go to movies and play dancing .at night I go to bad at 11 o'clock .\n",
    "# I like sunday .what do you do on sunday ?\n",
    "# I ' m 42 years old on sunday .\n",
    "# december 25th .I'm having a birthday party .\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 把標點符號edit token，變成after\n",
    "# 2. 簡化修改標記:  {+word+}, [-word-], [-word>>word+}\n",
    "# 3. 再斷句一次\n",
    "# 4. 把一句多錯誤，變成多句個含一個錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import fileinput, re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def simple_tag(tags):\n",
    "    if tags['d'] and tags['i']:    # d >> i\n",
    "        return '[-{d}>>{i}+}}'.format(d=tags['d'], i=tags['i'])\n",
    "    elif tags['d']:\n",
    "        return '[-{d}-]'.format(d=tags['d'])\n",
    "    elif tags['i']:\n",
    "        return '{{+{i}+}}'.format(i=tags['i'])\n",
    "    else:\n",
    "        print(\"Should not be here in simple_tag()\")\n",
    "\n",
    "re_tag = r'(\\[-(?P<d>.+)//(?P<d_tag>.+)-\\])?({\\+(?P<i>.+)//(?P<i_tag>.+)\\+})?'\n",
    "def correct_punc(line):\n",
    "    new_line = []\n",
    "    for token in line.split(' '):\n",
    "        tags = re.match(re_tag, token).groupdict()\n",
    "        if not tags['d_tag'] and not tags['i_tag']:  # no edit, 原字\n",
    "            new_line.append(token)\n",
    "        elif tags['i_tag'] == 'PU':                  # PU 錯誤類型不管，因此遇到 PU 則改成正確句子，只管被新增的符號\n",
    "            for item in tags['i'].split():           # TODO: 照原本寫法，不確定 split 用意\n",
    "                new_line.append(item)\n",
    "        elif tags['d_tag'] != 'PU':                  # error type not 'PU'\n",
    "            new_line.append(simple_tag(tags))   \n",
    "    return' '.join(new_line)\n",
    "\n",
    "def restore_line_break(text):\n",
    "    return text.replace('<br/>', '\\n').replace('<br>', '\\n').replace('<br />', '\\n')\n",
    "\n",
    "def restore_xmlescape(text):\n",
    "    while '&amp;' in text:\n",
    "        text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&quote;', '\"')\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text\n",
    "\n",
    "def mask_edits(text):\n",
    "    edits, tokens = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+') or token.startswith('[-'):\n",
    "            masked_token = \"{{{0}}}\".format(len(edits))\n",
    "            tokens.append(masked_token)\n",
    "            edits.append(token)\n",
    "        else:\n",
    "            tokens.append(token.replace('{', '{{').replace('}', '}}'))\n",
    "    return ' '.join(tokens), edits\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    text = restore_line_break(text)\n",
    "    text = restore_xmlescape(text)\n",
    "\n",
    "    # mask edit tokens first to prevent being segmented\n",
    "    # I have {+a+} pen. => I have {0} pen.\n",
    "    text_masked, edits = mask_edits(text)\n",
    "\n",
    "    for line in text_masked.splitlines():\n",
    "        for sent in sent_tokenize(line.strip()):\n",
    "            yield sent.format(*edits) \n",
    "\n",
    "def to_after(tokens):\n",
    "    def to_after_token(token):\n",
    "        token = token.replace('\\u3000', ' ')\n",
    "        if token.endswith('-]'):\n",
    "            return None\n",
    "        elif token.endswith('+}'):\n",
    "            return token[token.rfind('>>')+2:-2]  if token.startswith('[-') else token[2:-2]  \n",
    "        else:\n",
    "            return token\n",
    "    return ' '.join(token for token in map(to_after_token, tokens) if token)\n",
    "\n",
    "def to_single_edit_sents(sents):\n",
    "    for sent in sents:\n",
    "        for s in tokenize_doc(sent):\n",
    "            tokens =correct_punc(s).split(' ')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith('[-') or token.startswith('{+'):\n",
    "                    new_sent = to_after(tokens[:i]) + ' ' + token + ' ' + to_after(tokens[i+1:])\n",
    "                    yield new_sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"edit_type\": \"Insert\", \"sent\": \"{+I\\u3000am+} fine , thank you !\", \"edits\": [{\"Head\": {\"token\": \"am\", \"lemma\": \"be\", \"tag\": \"VBP\", \"dep\": null}, \"Target\": {\"token\": \"I\", \"lemma\": \"-PRON-\", \"tag\": \"PRP\", \"dep\": \"nsubj\"}, \"Child\": [], \"Delete\": []}, {\"Head\": {\"token\": \"am\", \"lemma\": \"be\", \"tag\": \"VBP\", \"dep\": null}, \"Target\": {\"token\": \"am\", \"lemma\": \"be\", \"tag\": \"VBP\", \"dep\": \"ROOT\"}, \"Child\": [{\"token\": \"I\", \"lemma\": \"-PRON-\", \"tag\": \"PRP\", \"dep\": \"nsubj\"}, {\"token\": \"fine\", \"lemma\": \"fine\", \"tag\": \"JJ\", \"dep\": \"acomp\"}, {\"token\": \",\", \"lemma\": \",\", \"tag\": \",\", \"dep\": \"punct\"}, {\"token\": \"thank\", \"lemma\": \"thank\", \"tag\": \"VBP\", \"dep\": \"conj\"}, {\"token\": \"!\", \"lemma\": \"!\", \"tag\": \".\", \"dep\": \"punct\"}], \"Delete\": []}]}\n",
      "{\"edit_type\": \"Replace\", \"sent\": \"I am [-Fine>>fine+} , thank you !\", \"edits\": [{\"Head\": {\"token\": \"am\", \"lemma\": \"be\", \"tag\": \"VBP\", \"dep\": null}, \"Target\": {\"token\": \"fine\", \"lemma\": \"fine\", \"tag\": \"JJ\", \"dep\": \"acomp\"}, \"Child\": [], \"Delete\": [{\"token\": \"Fine\", \"lemma\": \"fine\", \"tag\": \"JJ\"}]}]}\n",
      "{\"edit_type\": \"Insert\", \"sent\": \"Garlic and Echinacea tea : drink it when {+you+} have infection ,   it is simple but an excellent antibiotic .\", \"edits\": [{\"Head\": {\"token\": \"have\", \"lemma\": \"have\", \"tag\": \"VBP\", \"dep\": null}, \"Target\": {\"token\": \"you\", \"lemma\": \"-PRON-\", \"tag\": \"PRP\", \"dep\": \"nsubj\"}, \"Child\": [], \"Delete\": []}]}\n",
      "{\"edit_type\": \"Insert\", \"sent\": \"Garlic and Echinacea tea : drink it when you have infection ,   it is simple but {+an+} excellent antibiotic .\", \"edits\": [{\"Head\": {\"token\": \"simple\", \"lemma\": \"simple\", \"tag\": \"JJ\", \"dep\": null}, \"Target\": {\"token\": \"antibiotic\", \"lemma\": \"antibiotic\", \"tag\": \"NN\", \"dep\": \"conj\"}, \"Child\": [{\"token\": \"an\", \"lemma\": \"an\", \"tag\": \"DT\", \"dep\": \"det\"}, {\"token\": \"excellent\", \"lemma\": \"excellent\", \"tag\": \"JJ\", \"dep\": \"amod\"}], \"Delete\": []}]}\n",
      "{\"edit_type\": \"Insert\", \"sent\": \"Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains , itis {+a+} fast and effective way to relieve aches and pains .\", \"edits\": [{\"Head\": {\"token\": \"way\", \"lemma\": \"way\", \"tag\": \"NN\", \"dep\": null}, \"Target\": {\"token\": \"a\", \"lemma\": \"a\", \"tag\": \"DT\", \"dep\": \"det\"}, \"Child\": [], \"Delete\": []}]}\n",
      "{\"edit_type\": \"Replace\", \"sent\": \"[-everyone>>Everyone+} may use it in his or her daily life .\", \"edits\": [{\"Head\": {\"token\": \"use\", \"lemma\": \"use\", \"tag\": \"VB\", \"dep\": null}, \"Target\": {\"token\": \"Everyone\", \"lemma\": \"everyone\", \"tag\": \"NN\", \"dep\": \"nsubj\"}, \"Child\": [], \"Delete\": [{\"token\": \"everyone\", \"lemma\": \"everyone\", \"tag\": \"NN\"}]}]}\n",
      "{\"edit_type\": \"Insert\", \"sent\": \"Everyone may use it in {+his\\u3000or\\u3000her+} daily life .\", \"edits\": [{\"Head\": {\"token\": \"life\", \"lemma\": \"life\", \"tag\": \"NN\", \"dep\": null}, \"Target\": {\"token\": \"his\", \"lemma\": \"-PRON-\", \"tag\": \"PRP$\", \"dep\": \"poss\"}, \"Child\": [{\"token\": \"or\", \"lemma\": \"or\", \"tag\": \"CC\", \"dep\": \"cc\"}], \"Delete\": []}, {\"Head\": {\"token\": \"his\", \"lemma\": \"-PRON-\", \"tag\": \"PRP$\", \"dep\": null}, \"Target\": {\"token\": \"or\", \"lemma\": \"or\", \"tag\": \"CC\", \"dep\": \"cc\"}, \"Child\": [], \"Delete\": []}, {\"Head\": {\"token\": \"life\", \"lemma\": \"life\", \"tag\": \"NN\", \"dep\": null}, \"Target\": {\"token\": \"her\", \"lemma\": \"-PRON-\", \"tag\": \"PRP$\", \"dep\": \"poss\"}, \"Child\": [], \"Delete\": []}]}\n"
     ]
    }
   ],
   "source": [
    "# 用來抓 edit word\n",
    "re_words = r'(\\[-(?P<d>.+)-\\]|{\\+(?P<i>.+)\\+}|\\[-(?P<rd>.+)>>(?P<ri>.+)\\+})?'\n",
    "def correct(origin_tokens):\n",
    "    correct_tokens, pairs = [], []\n",
    "    for ot in origin_tokens:\n",
    "        ot = ot.replace('\\u3000', ' ')\n",
    "        words = re.match(re_words, ot).groupdict()\n",
    "        if words['rd'] and words['ri']:\n",
    "            pairs.append(('Replace', words['rd'], words['ri'], len(correct_tokens))) # 最後一欄位是對應 correct_tokens 用的\n",
    "            for ri in words['ri'].split():\n",
    "                correct_tokens.append(ri)\n",
    "        elif words['i']:\n",
    "            pairs.append(('Insert', \"\", words['i'], len(correct_tokens)))\n",
    "            for i in words['i'].split():\n",
    "                correct_tokens.append(i)\n",
    "        elif words['d']:\n",
    "            pairs.append(('Delete', words['d'], \"\", len(correct_tokens)))\n",
    "        else:\n",
    "            correct_tokens.append(ot)\n",
    "            \n",
    "    return correct_tokens, pairs\n",
    "\n",
    "    \n",
    "def format_edit(line, line_edits):\n",
    "    edit_type, origin_token, new_token, correct_token = line_edits[0]\n",
    "    \n",
    "    template = {\n",
    "        \"edit_type\": edit_type,\n",
    "        \"sent\": line,\n",
    "        \"edits\": []\n",
    "    }\n",
    "    \n",
    "    for e in line_edits:\n",
    "        edit_type, origin_token, new_token, correct_token = e\n",
    "        \n",
    "        for t in correct_token: # Insert or Replace\n",
    "            temp = {\n",
    "                \"Head\": {\n",
    "                    \"token\": t.head.text,\n",
    "                    \"lemma\": t.head.lemma_,\n",
    "                    \"tag\": t.head.tag_,\n",
    "                    \"dep\": None\n",
    "                },\n",
    "                \"Target\": {\n",
    "                    \"token\": t.text,\n",
    "                    \"lemma\": t.lemma_,\n",
    "                    \"tag\": t.tag_,\n",
    "                    \"dep\": t.dep_\n",
    "                },\n",
    "                \"Child\": [{\"token\": child.text, \"lemma\": child.lemma_, \"tag\": child.tag_, \"dep\": child.dep_} for child in t.children],\n",
    "                \"Delete\": [{\"token\": ot.text, \"lemma\": ot.lemma_, \"tag\": ot.tag_, \"dep\": None} for ot in nlp(origin_token)] if origin_token else []\n",
    "            }\n",
    "            template['edits'].append(temp)  \n",
    "\n",
    "    return template\n",
    "\n",
    "import json\n",
    "if __name__ == '__main__':\n",
    "    data = []\n",
    "#     for sent in to_single_edit_sents(test_data.split('\\n')):\n",
    "    for sent in to_single_edit_sents(fileinput.input()):\n",
    "        origin_tokens = sent.strip().split(' ')\n",
    "        correct_tokens, edit_pairs = correct(origin_tokens) # get edit pairs\n",
    "        if not correct_tokens or not edit_pairs: continue # skip no edit or empty string\n",
    "        \n",
    "        correct_tokens = list(filter(lambda x: x != '', correct_tokens))\n",
    "        correct_tokens = nlp(' '.join(correct_tokens))\n",
    "        \n",
    "        line_edits = []\n",
    "        for pair in edit_pairs: # 照理只有一個 pair\n",
    "            edit_type, origin_token, new_token, index = pair\n",
    "            \n",
    "            if edit_type == \"Delete\":\n",
    "                if index < len(correct_tokens):\n",
    "                    line_edits.append((edit_type, origin_token, new_token, correct_tokens[index-1:index]))\n",
    "                if index > 0:\n",
    "                    line_edits.append((edit_type, origin_token, new_token, correct_tokens[index:index+1]))\n",
    "            else:\n",
    "                line_edits.append((edit_type, origin_token, new_token, correct_tokens[index:index+len(new_token.split())]))\n",
    "\n",
    "        data.append(format_edit(sent, line_edits))\n",
    "    print(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
