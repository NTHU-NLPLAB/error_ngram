{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '''\n",
    "I {+'m+} fine , thanks you .\n",
    "I 'm [-a-] fine , thanks you .\n",
    "I [-am　going　to//AG-]{+will　quickly//AG+} take a bus .\n",
    "'''\n",
    "# Hello [-.//PU-]{+!//PU+} {+I　'm//MW+} [-Fine//C-]{+fine//C+} {+,//PU+} thanks {+.//PU+} [-and//C-]{+And//C+} you ?\n",
    "# Line up the bottles in rows of 4 , then 3 , then 2 , then 1 {+.//PU+} Get a frisbee per [-each//D-] player and allow to take two shots on each turn .\n",
    "# Each pin {+knocked　down//MW+} is one point .\n",
    "# {+For　a//MW+} [-Strike//C-]{+strike//C+} {+,//PU+} {+the　player//MW+} gets to take two more shots and [-add//AG-]{+adds//AG+} all the points together .\n",
    "# The player with the most points is {+the//AR+} winner .\n",
    "# Honey and ginger : [-it//PS-]{+they//PS+} [-is//AG-]{+are//AG+} [-natual//SP-]{+natural//SP+} food and good for sore throats [-,　no//PU-]{+,　no//PU+} side - effects , take a [-aspoonful//SP-]{+spoonful//SP+} anytime when you need .\n",
    "# Garlic and Echinacea tea : drink it when {+you//PS+} have infection [-,　it//PU-]{+,　it//PU+} is simple but {+an//AR+} excellent antibiotic .\n",
    "# Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains [-,　it//PU-]{+,　it//PU+} is {+a//AR+} fast and effective way to relieve aches and pains .\n",
    "# [-everyone//C-]{+Everyone//C+} may use it in {+his　or　her//PS+} daily life .\n",
    "# First , John asked Isabella not to marry him and [-giving　a　chance//XC-]{+her　to　give　him　the　chance//XC+} to prove himself to have {+the//AR+} ability to make [-the//AR-]{+a//AR+} happy life for her .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 把標點符號edit token，變成after\n",
    "# 2. 簡化修改標記:  {+word+}, [-word-], [-word>>word+}\n",
    "# 3. 再斷句一次\n",
    "# 4. 把一句多錯誤，變成多句個含一個錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I {+'m+} fine , thanks you .\",\n",
      " \"I 'm [-a-] fine , thanks you .\",\n",
      " 'I [-am\\u3000going\\u3000to>>will\\u3000quickly+} take a bus .']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import fileinput\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def simple_tag(tags):\n",
    "    if tags['d'] and tags['i']:    # d >> i\n",
    "        return '[-{d}>>{i}+}}'.format(d=tags['d'], i=tags['i'])\n",
    "    elif tags['d']:\n",
    "        return '[-{d}-]'.format(d=tags['d'])\n",
    "    elif tags['i']:\n",
    "        return '{{+{i}+}}'.format(i=tags['i'])\n",
    "    else:\n",
    "        print(\"Should not be here in simple_tag()\")\n",
    "\n",
    "re_tag = r'(\\[-(?P<d>.+)//(?P<d_tag>.+)-\\])?({\\+(?P<i>.+)//(?P<i_tag>.+)\\+})?'\n",
    "def correct_punc(line):\n",
    "    new_line = []\n",
    "    for token in line.split(' '):\n",
    "        tags = re.match(re_tag, token).groupdict()\n",
    "        if not tags['d_tag'] and not tags['i_tag']:  # no edit, 原字\n",
    "            new_line.append(token)\n",
    "        elif tags['i_tag'] == 'PU':                  # PU 錯誤類型不管，因此遇到 PU 則改成正確句子，只管被新增的符號\n",
    "            for item in tags['i'].split():           # TODO: 照原本寫法，不確定 split 用意\n",
    "                new_line.append(item)\n",
    "        elif tags['d_tag'] != 'PU':                  # error type not 'PU'\n",
    "            new_line.append(simple_tag(tags))   \n",
    "    return' '.join(new_line)\n",
    "\n",
    "def restore_line_break(text):\n",
    "    return text.replace('<br/>', '\\n').replace('<br>', '\\n').replace('<br />', '\\n')\n",
    "\n",
    "def restore_xmlescape(text):\n",
    "    while '&amp;' in text:\n",
    "        text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&quote;', '\"')\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text\n",
    "\n",
    "def mask_edits(text):\n",
    "    edits, tokens = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+') or token.startswith('[-'):\n",
    "            masked_token = \"{{{0}}}\".format(len(edits))\n",
    "            tokens.append(masked_token)\n",
    "            edits.append(token)\n",
    "        else:\n",
    "            tokens.append(token.replace('{', '{{').replace('}', '}}'))\n",
    "    return ' '.join(tokens), edits\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    text = restore_line_break(text)\n",
    "    text = restore_xmlescape(text)\n",
    "\n",
    "    # mask edit tokens first to prevent being segmented\n",
    "    # I have {+a+} pen. => I have {0} pen.\n",
    "    text_masked, edits = mask_edits(text)\n",
    "\n",
    "    for line in text_masked.splitlines():\n",
    "        for sent in sent_tokenize(line.strip()):\n",
    "            # restore masked edit tokens and return\n",
    "            yield sent.format(*edits) \n",
    "\n",
    "def to_after(tokens):\n",
    "    def to_after_token(token):\n",
    "        token = token.replace('\\u3000', ' ')\n",
    "        if token.endswith('-]'):\n",
    "            return None\n",
    "        elif token.endswith('+}'):\n",
    "            return token[token.rfind('>>')+2:-2]  if token.startswith('[-') else token[2:-2]  \n",
    "        else:\n",
    "            return token\n",
    "    return ' '.join(token for token in map(to_after_token, tokens) if token)\n",
    "\n",
    "new_data = []\n",
    "if __name__ == '__main__':\n",
    "    for line in test_data.split('\\n'): # fileinput.input():\n",
    "        simple_line = correct_punc(line.strip()) # remove PU\n",
    "        for sent in tokenize_doc(simple_line):\n",
    "            # after_sent = to_after(sent.split(' ')) # correct sentence\n",
    "            tokens = sent.split(' ')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith('[-') or token.startswith('{+'):\n",
    "                    new_sent = to_after(tokens[:i]) + ' ' + token + ' ' + to_after(tokens[i+1:])\n",
    "                    new_data.append(new_sent.strip())\n",
    "\n",
    "\n",
    "pprint(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "        (Insert) None\t->\t'm\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\t'm\t'm\tVBP\n",
      "        Target:\t'm\t'm\tVBP\tROOT\n",
      "        Child:\tI\t-PRON-\tPRP\tnsubj\n",
      "        Child:\tfine\tfine\tJJ\tacomp\n",
      "        Child:\t,\t,\t,\tpunct\n",
      "        Child:\tthanks\tthank\tNNS\tnpadvmod\n",
      "        Child:\t.\t.\t.\tpunct\n",
      "====================================\n",
      "        (Delete Preword) a\t->\tNone\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\t'm\t'm\tVBP\n",
      "        Target:\t'm\t'm\tVBP\tROOT\n",
      "        Child:\tI\t-PRON-\tPRP\tnsubj\n",
      "        Child:\tfine\tfine\tJJ\tacomp\n",
      "        Child:\t,\t,\t,\tpunct\n",
      "        Child:\tthanks\tthank\tNNS\tnpadvmod\n",
      "        Child:\t.\t.\t.\tpunct\n",
      "====================================\n",
      "        (Delete Postword) a\t->\tNone\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\t'm\t'm\tVBP\n",
      "        Target:\tfine\tfine\tJJ\tacomp\n",
      "====================================\n",
      "        (Replace) am\t->\twill\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\twill\twill\tMD\taux\n",
      "====================================\n",
      "        (Replace) going\t->\twill\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\twill\twill\tMD\taux\n",
      "====================================\n",
      "        (Replace) to\t->\twill\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\twill\twill\tMD\taux\n",
      "====================================\n",
      "        (Replace) am\t->\tquickly\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\tquickly\tquickly\tRB\tadvmod\n",
      "====================================\n",
      "        (Replace) going\t->\tquickly\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\tquickly\tquickly\tRB\tadvmod\n",
      "====================================\n",
      "        (Replace) to\t->\tquickly\n",
      "        \n",
      "        \tToken\tLemma\tTag\tDep(to head)\n",
      "        Head:\ttake\ttake\tVB\n",
      "        Target:\tquickly\tquickly\tRB\tadvmod\n"
     ]
    }
   ],
   "source": [
    "# 用來抓 edit word\n",
    "re_words = r'(\\[-(?P<d>.+)-\\]|{\\+(?P<i>.+)\\+}|\\[-(?P<rd>.+)>>(?P<ri>.+)\\+})?'\n",
    "def correct(origin_tokens):\n",
    "    correct_tokens, pairs = [], []\n",
    "    for ot in origin_tokens:\n",
    "        ot = ot.replace('\\u3000', ' ')\n",
    "        words = re.match(re_words, ot).groupdict()\n",
    "        if words['rd'] and words['ri']:\n",
    "            for ri in words['ri'].split():\n",
    "                for rd in words['rd'].split():\n",
    "                    pairs.append(('Replace', rd, ri, len(correct_tokens))) # 最後一欄位是對應 correct_tokens 用的\n",
    "                correct_tokens.append(ri)\n",
    "        elif words['i']:\n",
    "            for i in words['i'].split():\n",
    "                pairs.append(('Insert', None, i, len(correct_tokens)))\n",
    "                correct_tokens.append(i)\n",
    "        elif words['d']:\n",
    "            pairs.append(('Delete', words['d'], None, len(correct_tokens)))\n",
    "        else:\n",
    "            correct_tokens.append(ot)\n",
    "            \n",
    "    return correct_tokens, pairs\n",
    "\n",
    "    \n",
    "def format_edit(edit):\n",
    "    edit_type, origin_token, new_token, correct_token = edit\n",
    "    \n",
    "    template = '''====================================\n",
    "        ({edit_type}) {origin_token}\\t->\\t{new_token}\n",
    "        \n",
    "        \\tToken\\tLemma\\tTag\\tDep(to head)\n",
    "        Head:\\t{head}\\t{head_lemma}\\t{head_tag}\n",
    "        Target:\\t{target_token}\\t{target_token_lemma}\\t{target_token_tag}\\t{target_token_dep}'''.format(\n",
    "               edit_type = edit_type, \n",
    "               origin_token = origin_token,\n",
    "               new_token = new_token,\n",
    "               target_token = correct_token.text,\n",
    "               target_token_lemma = correct_token.lemma_,\n",
    "               target_token_tag = correct_token.tag_,\n",
    "               target_token_dep = correct_token.dep_,\n",
    "                   \n",
    "               head = correct_token.head.text,\n",
    "               head_lemma = correct_token.head.lemma_,\n",
    "               head_tag = correct_token.head.tag_)\n",
    "    \n",
    "    for child in correct_token.children:\n",
    "        template += '''\n",
    "        Child:\\t{child}\\t{child_lemma}\\t{child_tag}\\t{child_dep}'''.format(\n",
    "            child = child.text, child_lemma = child.lemma_, child_tag = child.tag_, child_dep = child.dep_)\n",
    "    return template\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    all_edits = []\n",
    "    for line in new_data: # fileinput.input():\n",
    "        origin_tokens = line.strip().split(' ')\n",
    "        correct_tokens, pairs = correct(origin_tokens)\n",
    "        if not correct_tokens or not pairs: continue # skip no edit or empty string\n",
    "            \n",
    "        correct_tokens = nlp(' '.join(correct_tokens))\n",
    "        for pair in pairs:\n",
    "            edit_type, origin_token, new_token, index = pair\n",
    "            if edit_type == \"Delete\":\n",
    "                if index < len(correct_tokens):\n",
    "                    all_edits.append((edit_type + \" Preword\", origin_token, new_token, correct_tokens[index-1]))\n",
    "                if index > 0:\n",
    "                    all_edits.append((edit_type + \" Postword\", origin_token, new_token, correct_tokens[index]))\n",
    "            else:\n",
    "                all_edits.append((edit_type, origin_token, new_token, correct_tokens[index]))\n",
    "    \n",
    "    for edit in all_edits:\n",
    "        print(format_edit(edit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re_words = r'(\\[-(?P<d>.+)-\\]|{\\+(?P<i>.+)\\+}|\\[-(?P<rd>.+)>>(?P<ri>.+)\\+})?'\n",
    "# def diff2before_after(line):\n",
    "#     alignment_aft, alignment_bef = {}, {}\n",
    "#     before, after = [], []\n",
    "#     for i, token in enumerate(line.split(' ')):\n",
    "#         token = token.replace('\\u3000', ' ') # fullwidth -> halfwidth\n",
    "#         words = re.match(re_words, token).groupdict()\n",
    "#         # TODO: 可以改寫成兩個 if\n",
    "#         if words['i']: # starts with '{+'\n",
    "#             for item in words['i'].split():\n",
    "#                 after.append(item)\n",
    "#                 alignment_aft[len(after)-1] = i\n",
    "#         elif words['d']:\n",
    "#             for item in words['d'].split():\n",
    "#                 before.append(item)\n",
    "#                 alignment_bef[len(before)-1] = i\n",
    "#         elif words['rd'] and words['ri']: # '[-rd>>ri+}'\n",
    "#             for item in words['ri'].split():\n",
    "#                 after.append(item)\n",
    "#                 alignment_aft[len(after)-1] = i\n",
    "#             for item in words['rd'].split():\n",
    "#                 before.append(item)\n",
    "#                 alignment_bef[len(before)-1] = i\n",
    "#         else:\n",
    "#             before.append(token)\n",
    "#             after.append(token)\n",
    "#             alignment_aft[len(after)-1] = i\n",
    "#             alignment_bef[len(before)-1] = i\n",
    "            \n",
    "#     return before, alignment_bef, after, alignment_aft\n",
    "\n",
    "# def spacy_aft(after, nlp):\n",
    "#     doc_aft = nlp(' '.join(after))\n",
    "#     childs, childs_texts = [], []\n",
    "#     for i, token in enumerate(doc_aft):\n",
    "#         lemmas      = [token.lemma_    for token in doc_aft]\n",
    "#         tags        = [token.tag_      for token in doc_aft]\n",
    "#         deps        = [token.dep_      for token in doc_aft]\n",
    "#         heads       = [token.head.text for token in doc_aft]\n",
    "#         head_lemmas = [token.head.lemma_ for token in doc_aft]\n",
    "#         head_tags   = [token.head.tag_ for token in doc_aft]\n",
    "        \n",
    "#         childs.append([child for child in token.children])\n",
    "#         for i, child_text in enumerate(childs): \n",
    "#             child_subtext = []\n",
    "#             if child_text:\n",
    "#                 for char in child_text: # spacy to string\n",
    "#                     child_subtext.append(str(char))\n",
    "#         childs_texts.append(child_subtext)\n",
    "#     return lemmas, tags, deps, heads, head_lemmas, head_tags, childs, childs_texts\n",
    "\n",
    "\n",
    "# def spacy_bef(before, nlp):\n",
    "#     doc_bef = nlp(' '.join(before))\n",
    "#     for i, token in enumerate(doc_bef):\n",
    "#         lemmas_bef = [token.lemma_ for token in doc_bef]\n",
    "#         tags_bef = [token.tag_ for token in doc_bef]\n",
    "#         return lemmas_bef, tags_bef\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     for line in new_data: # fileinput.input():\n",
    "#         line = line.strip()\n",
    "        \n",
    "#         before_tokens, alignment_bef, after_tokens, alignment_aft = diff2before_after(line)\n",
    "#         if after_tokens:\n",
    "#             lemmas, tags, deps, heads, head_lemmas, head_tags, childs, childs_texts = spacy_aft(after_tokens, nlp)\n",
    "#         if before_tokens:\n",
    "#             lemmas_bef, tags_bef = spacy_bef(before_tokens, nlp)\n",
    "            \n",
    "#         for i, token in enumerate(after_tokens):\n",
    "#             diff_token = line.split(' ')[alignment_aft[i]]\n",
    "#             if diff_token.endswith('+}'): # 先找到 target word 然後搜集相關資料\n",
    "#                 edit_spacy = '(' + after_tokens[i] + ')' + lemmas[i] + '_' + tags[i]\n",
    "#                 head_spacy = head_tags[i] + '_' +  head_lemmas[i] + '('+ heads[i] + ')'\n",
    "#                 edit_head_dep = '||' + deps[i] + '::'\n",
    "#                 if diff_token.startswith('[-'):\n",
    "#                     for j, token in enumerate(before_tokens):\n",
    "#                         if alignment_bef[j] == alignment_aft[i]:\n",
    "#                             edit_bef_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "#                             print('RepH: ' + edit_spacy + '[' + edit_bef_spacy + ']' + edit_head_dep + head_spacy)\n",
    "#                 else: \n",
    "#                     print('InsH: ' + edit_spacy + edit_head_dep + head_spacy)\n",
    "            \n",
    "#             for k, diff_token in enumerate(line.split(' ')):\n",
    "#                 pre_token_spacys = []\n",
    "#                 post_token_spacys = []\n",
    "#                 if diff_token.endswith('-]'):\n",
    "#                     if alignment_aft[i]+1 == k:\n",
    "#                         if i > 1:\n",
    "#                             pre_spacy = '(' + after_tokens[i] + ')' + lemmas[i] + '_' + tags[i]\n",
    "#                             pre_head_spacy = head_tags[i] + '_' +  head_lemmas[i] + '('+ heads[i] + ')'\n",
    "#                             pre_head_dep = '||' + deps[i] + '::'\n",
    "#                             pre_token_spacys.append(pre_spacy + pre_head_dep + pre_head_spacy)\n",
    "                            \n",
    "#                         if i < len(after_tokens)-1:\n",
    "#                             post_spacy = '(' + after_tokens[i+1] + ')' + lemmas[i+1] + '_' + tags[i+1]\n",
    "#                             post_head_spacy = head_tags[i+1] + '_' +  head_lemmas[i+1] + '('+ heads[i+1] + ')'\n",
    "#                             post_head_dep = '||' + deps[i+1] + '::'\n",
    "#                             post_token_spacys.append(post_spacy + post_head_dep + post_head_spacy)\n",
    "                            \n",
    "#                         for j, token in enumerate(before_tokens):\n",
    "#                             if alignment_bef[j] == k:\n",
    "#                                 edit_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "#                                 print('D: ' + ''.join(pre_token_spacys) + '[' + edit_spacy + ']' + ''.join(post_token_spacys))\n",
    "# #                 print(\"pre\", pre_token_spacys)\n",
    "# #                 print(\"post\", post_token_spacys)\n",
    "                \n",
    "#             head_is = [head_i for head_i, token in enumerate(after_tokens) if token == heads[i]] # 指向自己？\n",
    "\n",
    "#             for head_i in head_is:\n",
    "#                 if after_tokens[i] in childs_texts[head_i]:  \n",
    "#                     diff_token_h = line.split(' ')[alignment_aft[head_i]]\n",
    "#                     if diff_token_h.endswith('+}'):\n",
    "#                         edit_spacy = '(' + after_tokens[head_i] + ')' + lemmas[head_i] + '_' + tags[head_i]\n",
    "#                         child_spacy = tags[i] + '_' + lemmas[i] + '('+ after_tokens[i] + ')'\n",
    "#                         edit_child_dep = '::' + deps[i] + '||'\n",
    "#                         if diff_token_h.startswith('[-'):\n",
    "#                             for j, token in enumerate(before_tokens):\n",
    "#                                 if alignment_bef[j] == alignment_aft[head_i]:\n",
    "#                                     edit_bef_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "#                                     # return 'replace; child'\n",
    "#                                     print('RepC: ' + edit_spacy + '[' + edit_bef_spacy + ']' + edit_child_dep + child_spacy)\n",
    "#                         else:\n",
    "#                             # return 'insert; child'\n",
    "#                             print('InsC: ' + edit_spacy + edit_child_dep + child_spacy)\n",
    "#         print('')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
