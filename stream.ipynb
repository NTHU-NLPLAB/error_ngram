{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '''\n",
    "Hello [-.//PU-]{+!//PU+} {+I　'm//MW+} [-Fine//C-]{+fine//C+} {+,//PU+} thanks {+.//PU+} [-and//C-]{+And//C+} you ?\n",
    "Line up the bottles in rows of 4 , then 3 , then 2 , then 1 {+.//PU+} Get a frisbee per [-each//D-] player and allow to take two shots on each turn .\n",
    "Each pin {+knocked　down//MW+} is one point .\n",
    "{+For　a//MW+} [-Strike//C-]{+strike//C+} {+,//PU+} {+the　player//MW+} gets to take two more shots and [-add//AG-]{+adds//AG+} all the points together .\n",
    "The player with the most points is {+the//AR+} winner .\n",
    "Honey and ginger : [-it//PS-]{+they//PS+} [-is//AG-]{+are//AG+} [-natual//SP-]{+natural//SP+} food and good for sore throats [-,　no//PU-]{+,　no//PU+} side - effects , take a [-aspoonful//SP-]{+spoonful//SP+} anytime when you need .\n",
    "Garlic and Echinacea tea : drink it when {+you//PS+} have infection [-,　it//PU-]{+,　it//PU+} is simple but {+an//AR+} excellent antibiotic .\n",
    "Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains [-,　it//PU-]{+,　it//PU+} is {+a//AR+} fast and effective way to relieve aches and pains .\n",
    "[-everyone//C-]{+Everyone//C+} may use it in {+his　or　her//PS+} daily life .\n",
    "First , John asked Isabella not to marry him and [-giving　a　chance//XC-]{+her　to　give　him　the　chance//XC+} to prove himself to have {+the//AR+} ability to make [-the//AR-]{+a//AR+} happy life for her .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 把標點符號edit token，變成after\n",
    "# 2. 簡化修改標記:  {+word+}, [-word-], [-word>>word+}\n",
    "# 3. 再斷句一次\n",
    "# 4. 把一句多錯誤，變成多句個含一個錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{+I\\u3000'm+} fine , thanks .\",\n",
      " \"I 'm [-Fine>>fine+} , thanks .\",\n",
      " '[-and>>And+} you ?',\n",
      " 'Get a frisbee per [-each-] player and allow to take two shots on each turn .',\n",
      " 'Each pin {+knocked\\u3000down+} is one point .',\n",
      " '{+For\\u3000a+} strike , the player gets to take two more shots and adds all '\n",
      " 'the points together .',\n",
      " 'For a [-Strike>>strike+} , the player gets to take two more shots and adds '\n",
      " 'all the points together .',\n",
      " 'For a strike , {+the\\u3000player+} gets to take two more shots and adds all '\n",
      " 'the points together .',\n",
      " 'For a strike , the player gets to take two more shots and [-add>>adds+} all '\n",
      " 'the points together .',\n",
      " 'The player with the most points is {+the+} winner .',\n",
      " 'Honey and ginger : [-it>>they+} are natural food and good for sore throats , '\n",
      " 'no side - effects , take a spoonful anytime when you need .',\n",
      " 'Honey and ginger : they [-is>>are+} natural food and good for sore throats , '\n",
      " 'no side - effects , take a spoonful anytime when you need .',\n",
      " 'Honey and ginger : they are [-natual>>natural+} food and good for sore '\n",
      " 'throats , no side - effects , take a spoonful anytime when you need .',\n",
      " 'Honey and ginger : they are natural food and good for sore throats , no side '\n",
      " '- effects , take a [-aspoonful>>spoonful+} anytime when you need .',\n",
      " 'Garlic and Echinacea tea : drink it when {+you+} have infection , it is '\n",
      " 'simple but an excellent antibiotic .',\n",
      " 'Garlic and Echinacea tea : drink it when you have infection , it is simple '\n",
      " 'but {+an+} excellent antibiotic .',\n",
      " 'Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and '\n",
      " 'pains , it is {+a+} fast and effective way to relieve aches and pains .',\n",
      " '[-everyone>>Everyone+} may use it in his or her daily life .',\n",
      " 'Everyone may use it in {+his\\u3000or\\u3000her+} daily life .',\n",
      " 'First , John asked Isabella not to marry him and [-giving\\u3000a\\u3000'\n",
      " 'chance>>her\\u3000to\\u3000give\\u3000him\\u3000the\\u3000chance+} to prove '\n",
      " 'himself to have the ability to make a happy life for her .',\n",
      " 'First , John asked Isabella not to marry him and her to give him the chance '\n",
      " 'to prove himself to have {+the+} ability to make a happy life for her .',\n",
      " 'First , John asked Isabella not to marry him and her to give him the chance '\n",
      " 'to prove himself to have the ability to make [-the>>a+} happy life for her .']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import fileinput\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def simple_tag(tags):\n",
    "    if tags['d'] and tags['i']:    # d >> i\n",
    "        return '[-{d}>>{i}+}}'.format(d=tags['d'], i=tags['i'])\n",
    "    elif tags['d']:\n",
    "        return '[-{d}-]'.format(d=tags['d'])\n",
    "    elif tags['i']:\n",
    "        return '{{+{i}+}}'.format(i=tags['i'])\n",
    "    else:\n",
    "        print(\"Should not be here in simple_tag()\")\n",
    "\n",
    "re_tag = r'(\\[-(?P<d>.+)//(?P<d_tag>.+)-\\])?({\\+(?P<i>.+)//(?P<i_tag>.+)\\+})?'\n",
    "def correct_punc(line):\n",
    "    new_line = []\n",
    "    for token in line.split(' '):\n",
    "        tags = re.match(re_tag, token).groupdict()\n",
    "        if not tags['d_tag'] and not tags['i_tag']:  # no edit, 原字\n",
    "            new_line.append(token)\n",
    "        elif tags['i_tag'] == 'PU':                  # PU 錯誤類型不管，因此遇到 PU 則改成正確句子，只管被新增的符號\n",
    "            for item in tags['i'].split():           # TODO: 照原本寫法，不確定 split 用意\n",
    "                new_line.append(item)\n",
    "        elif tags['d_tag'] != 'PU':                  # error type not 'PU'\n",
    "            new_line.append(simple_tag(tags))   \n",
    "    return' '.join(new_line)\n",
    "\n",
    "def restore_line_break(text):\n",
    "    return text.replace('<br/>', '\\n').replace('<br>', '\\n').replace('<br />', '\\n')\n",
    "\n",
    "def restore_xmlescape(text):\n",
    "    while '&amp;' in text:\n",
    "        text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&quote;', '\"')\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text\n",
    "\n",
    "def mask_edits(text):\n",
    "    edits, tokens = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+') or token.startswith('[-'):\n",
    "            masked_token = \"{{{0}}}\".format(len(edits))\n",
    "            tokens.append(masked_token)\n",
    "            edits.append(token)\n",
    "        else:\n",
    "            tokens.append(token.replace('{', '{{').replace('}', '}}'))\n",
    "    return ' '.join(tokens), edits\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    text = restore_line_break(text)\n",
    "    text = restore_xmlescape(text)\n",
    "\n",
    "    # mask edit tokens first to prevent being segmented\n",
    "    # I have {+a+} pen. => I have {0} pen.\n",
    "    text_masked, edits = mask_edits(text)\n",
    "\n",
    "    for line in text_masked.splitlines():\n",
    "        for sent in sent_tokenize(line.strip()):\n",
    "            # restore masked edit tokens and return\n",
    "            yield sent.format(*edits) \n",
    "\n",
    "def to_after(tokens):\n",
    "    def to_after_token(token):\n",
    "        token = token.replace('\\u3000', ' ')\n",
    "        if token.endswith('-]'):\n",
    "            return None\n",
    "        elif token.endswith('+}'):\n",
    "            return token[token.rfind('>>')+2:-2]  if token.startswith('[-') else token[2:-2]  \n",
    "        else:\n",
    "            return token\n",
    "    return ' '.join(token for token in map(to_after_token, tokens) if token)\n",
    "\n",
    "new_data = []\n",
    "if __name__ == '__main__':\n",
    "    for line in test_data.split('\\n'): # fileinput.input():\n",
    "        simple_line = correct_punc(line.strip()) # remove PU\n",
    "        for sent in tokenize_doc(simple_line):\n",
    "            # after_sent = to_after(sent.split(' ')) # correct sentence\n",
    "            tokens = sent.split(' ')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith('[-') or token.startswith('{+'):\n",
    "                    new_sent = to_after(tokens[:i]) + ' ' + token + ' ' + to_after(tokens[i+1:])\n",
    "                    new_data.append(new_sent.strip())\n",
    "\n",
    "\n",
    "pprint(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InsH: (I)-PRON-_PRP||nsubj::VBP_'m('m)\n",
      "InsC: ('m)'m_VBP::nsubj||PRP_-PRON-(I)\n",
      "InsH: ('m)'m_VBP||ROOT::VBP_'m('m)\n",
      "InsC: ('m)'m_VBP::acomp||JJ_fine(fine)\n",
      "InsC: ('m)'m_VBP::punct||,_,(,)\n",
      "InsC: ('m)'m_VBP::npadvmod||NNS_thank(thanks)\n",
      "InsC: ('m)'m_VBP::punct||._.(.)\n",
      "\n",
      "RepH: (fine)fine_JJ[(Fine)fine_JJ]||acomp::VBP_'m('m)\n",
      "\n",
      "RepH: (And)and_CC[(and)and_CC]||cc::PRP_-PRON-(you)\n",
      "\n",
      "D: (per)per_IN||prep::NN_frisbee(frisbee)[(each)each_DT](player)player_NN||pobj::IN_per(per)\n",
      "\n",
      "InsC: (knocked)knock_VBD::nsubj||NN_pin(pin)\n",
      "InsH: (knocked)knock_VBD||csubj::VBZ_be(is)\n",
      "InsH: (down)down_RP||prt::VBD_knock(knocked)\n",
      "InsC: (knocked)knock_VBD::prt||RP_down(down)\n",
      "\n",
      "InsH: (For)for_IN||prep::VBZ_get(gets)\n",
      "InsH: (a)a_DT||det::NN_strike(strike)\n",
      "InsC: (For)for_IN::pobj||NN_strike(strike)\n",
      "\n",
      "RepC: (strike)strike_NN[(Strike)strike_NN]::det||DT_a(a)\n",
      "RepH: (strike)strike_NN[(Strike)strike_NN]||pobj::IN_for(For)\n",
      "\n",
      "InsH: (the)the_DT||det::NN_player(player)\n",
      "InsC: (player)player_NN::det||DT_the(the)\n",
      "InsH: (player)player_NN||nsubj::VBZ_get(gets)\n",
      "\n",
      "RepH: (adds)add_VBZ[(add)add_VB]||conj::VBZ_get(gets)\n",
      "RepC: (adds)add_VBZ[(add)add_VB]::dobj||NNS_point(points)\n",
      "RepC: (adds)add_VBZ[(add)add_VB]::advmod||RB_together(together)\n",
      "\n",
      "InsH: (the)the_DT||det::NN_winner(winner)\n",
      "\n",
      "RepH: (they)-PRON-_PRP[(it)-PRON-_PRP]||nsubj::VBP_be(are)\n",
      "\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::dep||NNP_honey(Honey)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::punct||:_:(:)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::nsubj||PRP_-PRON-(they)\n",
      "RepH: (are)be_VBP[(is)be_VBZ]||ROOT::VBP_be(are)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::attr||NN_food(food)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::punct||,_,(,)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::conj||VB_take(take)\n",
      "RepC: (are)be_VBP[(is)be_VBZ]::punct||._.(.)\n",
      "\n",
      "RepH: (natural)natural_JJ[(natual)natual_JJ]||amod::NN_food(food)\n",
      "\n",
      "RepH: (spoonful)spoonful_JJ[(aspoonful)aspoonful_JJ]||amod::NN_anytime(anytime)\n",
      "\n",
      "InsH: (you)-PRON-_PRP||nsubj::VBP_have(have)\n",
      "\n",
      "InsH: (an)an_DT||det::NN_antibiotic(antibiotic)\n",
      "\n",
      "InsH: (a)a_DT||det::NN_way(way)\n",
      "\n",
      "RepH: (Everyone)everyone_NN[(everyone)everyone_NN]||nsubj::VB_use(use)\n",
      "\n",
      "InsH: (his)-PRON-_PRP$||poss::NN_life(life)\n",
      "InsH: (or)or_CC||cc::PRP$_-PRON-(his)\n",
      "InsC: (his)-PRON-_PRP$::cc||CC_or(or)\n",
      "InsH: (her)-PRON-_PRP$||poss::NN_life(life)\n",
      "\n",
      "RepH: (her)-PRON-_PRP[(giving)give_VBG]||conj::PRP_-PRON-(him)\n",
      "RepH: (her)-PRON-_PRP[(a)a_DT]||conj::PRP_-PRON-(him)\n",
      "RepH: (her)-PRON-_PRP[(chance)chance_NN]||conj::PRP_-PRON-(him)\n",
      "RepH: (to)to_TO[(giving)give_VBG]||aux::VB_give(give)\n",
      "RepH: (to)to_TO[(a)a_DT]||aux::VB_give(give)\n",
      "RepH: (to)to_TO[(chance)chance_NN]||aux::VB_give(give)\n",
      "RepC: (give)give_VB[(giving)give_VBG]::aux||TO_to(to)\n",
      "RepC: (give)give_VB[(a)a_DT]::aux||TO_to(to)\n",
      "RepC: (give)give_VB[(chance)chance_NN]::aux||TO_to(to)\n",
      "RepH: (give)give_VB[(giving)give_VBG]||advcl::VB_marry(marry)\n",
      "RepH: (give)give_VB[(a)a_DT]||advcl::VB_marry(marry)\n",
      "RepH: (give)give_VB[(chance)chance_NN]||advcl::VB_marry(marry)\n",
      "RepH: (him)-PRON-_PRP[(giving)give_VBG]||dative::VB_give(give)\n",
      "RepH: (him)-PRON-_PRP[(a)a_DT]||dative::VB_give(give)\n",
      "RepH: (him)-PRON-_PRP[(chance)chance_NN]||dative::VB_give(give)\n",
      "RepC: (give)give_VB[(giving)give_VBG]::dative||PRP_-PRON-(him)\n",
      "RepC: (give)give_VB[(a)a_DT]::dative||PRP_-PRON-(him)\n",
      "RepC: (give)give_VB[(chance)chance_NN]::dative||PRP_-PRON-(him)\n",
      "RepH: (the)the_DT[(giving)give_VBG]||det::NN_chance(chance)\n",
      "RepH: (the)the_DT[(a)a_DT]||det::NN_chance(chance)\n",
      "RepH: (the)the_DT[(chance)chance_NN]||det::NN_chance(chance)\n",
      "RepC: (chance)chance_NN[(giving)give_VBG]::det||DT_the(the)\n",
      "RepC: (chance)chance_NN[(a)a_DT]::det||DT_the(the)\n",
      "RepC: (chance)chance_NN[(chance)chance_NN]::det||DT_the(the)\n",
      "RepH: (chance)chance_NN[(giving)give_VBG]||dobj::VB_give(give)\n",
      "RepH: (chance)chance_NN[(a)a_DT]||dobj::VB_give(give)\n",
      "RepH: (chance)chance_NN[(chance)chance_NN]||dobj::VB_give(give)\n",
      "RepC: (give)give_VB[(giving)give_VBG]::dobj||NN_chance(chance)\n",
      "RepC: (give)give_VB[(a)a_DT]::dobj||NN_chance(chance)\n",
      "RepC: (give)give_VB[(chance)chance_NN]::dobj||NN_chance(chance)\n",
      "RepC: (chance)chance_NN[(giving)give_VBG]::acl||VB_prove(prove)\n",
      "RepC: (chance)chance_NN[(a)a_DT]::acl||VB_prove(prove)\n",
      "RepC: (chance)chance_NN[(chance)chance_NN]::acl||VB_prove(prove)\n",
      "\n",
      "InsH: (the)the_DT||det::NN_ability(ability)\n",
      "\n",
      "RepH: (a)a_DT[(the)the_DT]||det::NN_life(life)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "re_words = r'(\\[-(?P<d>.+)-\\]|{\\+(?P<i>.+)\\+}|\\[-(?P<rd>.+)>>(?P<ri>.+)\\+})?'\n",
    "def diff2before_after(line):\n",
    "    alignment_aft, alignment_bef = {}, {}\n",
    "    before, after = [], []\n",
    "    for i, token in enumerate(line.split(' ')):\n",
    "        token = token.replace('\\u3000', ' ') # fullwidth -> halfwidth\n",
    "        words = re.match(re_words, token).groupdict()\n",
    "        # TODO: 可以改寫成兩個 if\n",
    "        if words['i']: # starts with '{+'\n",
    "            for item in words['i'].split():\n",
    "                after.append(item)\n",
    "                alignment_aft[len(after)-1] = i\n",
    "        elif words['rd'] and words['ri']: # '[-rd>>ri+}'\n",
    "            for item in words['ri'].split():\n",
    "                after.append(item)\n",
    "                alignment_aft[len(after)-1] = i\n",
    "            for item in words['rd'].split():\n",
    "                before.append(item)\n",
    "                alignment_bef[len(before)-1] = i\n",
    "        elif words['d']:\n",
    "            for item in words['d'].split():\n",
    "                before.append(item)\n",
    "                alignment_bef[len(before)-1] = i\n",
    "        else:\n",
    "            before.append(token)\n",
    "            after.append(token)\n",
    "            alignment_aft[len(after)-1] = i\n",
    "            alignment_bef[len(before)-1] = i\n",
    "    return before, alignment_bef, after, alignment_aft\n",
    "\n",
    "def spacy_aft(after, nlp):\n",
    "    doc_aft = nlp(' '.join(after))\n",
    "    childs, childs_texts = [], []\n",
    "    for i, token in enumerate(doc_aft):\n",
    "        lemmas = [token.lemma_    for token in doc_aft]\n",
    "        tags   = [token.tag_      for token in doc_aft]\n",
    "        deps   = [token.dep_      for token in doc_aft]\n",
    "        heads  = [token.head.text for token in doc_aft]\n",
    "        head_lemmas = [token.head.lemma_ for token in doc_aft]\n",
    "        head_tags = [token.head.tag_ for token in doc_aft]\n",
    "        childs.append([child for child in token.children])\n",
    "        for i, child_text in enumerate(childs): \n",
    "            child_subtext = []\n",
    "            if child_text:\n",
    "                for char in child_text:\n",
    "                    child_subtext.append(str(char))\n",
    "        childs_texts.append(child_subtext)\n",
    "    return lemmas, tags, deps, heads, head_lemmas, head_tags, childs, childs_texts\n",
    "\n",
    "\n",
    "def spacy_bef(before, nlp):\n",
    "    doc_bef = nlp(' '.join(before))\n",
    "    for i, token in enumerate(doc_bef):\n",
    "        lemmas_bef = [token.lemma_ for token in doc_bef]\n",
    "        tags_bef = [token.tag_ for token in doc_bef]\n",
    "        return lemmas_bef, tags_bef\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for line in new_data: # fileinput.input():\n",
    "        line = line.strip()\n",
    "        \n",
    "        before_tokens, alignment_bef, after_tokens, alignment_aft = diff2before_after(line)\n",
    "        if after_tokens:\n",
    "            lemmas, tags, deps, heads, head_lemmas, head_tags, childs, childs_texts = spacy_aft(after_tokens, nlp)\n",
    "        if before_tokens:\n",
    "            lemmas_bef, tags_bef = spacy_bef(before_tokens, nlp)\n",
    "\n",
    "        for i, token in enumerate(after_tokens):\n",
    "            diff_token = line.split(' ')[alignment_aft[i]]\n",
    "#             print(diff_token)\n",
    "            head_is = [head_i for head_i, token in enumerate(after_tokens) if token == heads[i]]\n",
    "#             print(head_is)\n",
    "            if diff_token.endswith('+}'):\n",
    "                edit_spacy = '(' + after_tokens[i] + ')' + lemmas[i] + '_' + tags[i]\n",
    "                head_spacy = head_tags[i] + '_' +  head_lemmas[i] + '('+ heads[i] + ')'\n",
    "                edit_head_dep = '||' + deps[i] + '::'\n",
    "                if diff_token.startswith('[-'):\n",
    "                    for j, token in enumerate(before_tokens):\n",
    "                        if alignment_bef[j] == alignment_aft[i]:\n",
    "                            edit_bef_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "                            print('RepH: ' + edit_spacy + '[' + edit_bef_spacy + ']' + edit_head_dep + head_spacy)\n",
    "                else: \n",
    "                    print('InsH: ' + edit_spacy + edit_head_dep + head_spacy)\n",
    "            \n",
    "            for k, diff_token in enumerate(line.split(' ')):\n",
    "                pre_token_spacys = []\n",
    "                post_token_spacys = []\n",
    "                if diff_token.endswith('-]'):\n",
    "                    if alignment_aft[i]+1 == k:\n",
    "                        if i > 1:\n",
    "                            pre_spacy = '(' + after_tokens[i] + ')' + lemmas[i] + '_' + tags[i]\n",
    "                            pre_head_spacy = head_tags[i] + '_' +  head_lemmas[i] + '('+ heads[i] + ')'\n",
    "                            pre_head_dep = '||' + deps[i] + '::'\n",
    "                            pre_token_spacys.append(pre_spacy + pre_head_dep + pre_head_spacy)\n",
    "                            \n",
    "                        if i < len(after_tokens)-1:\n",
    "                            post_spacy = '(' + after_tokens[i+1] + ')' + lemmas[i+1] + '_' + tags[i+1]\n",
    "                            post_head_spacy = head_tags[i+1] + '_' +  head_lemmas[i+1] + '('+ heads[i+1] + ')'\n",
    "                            post_head_dep = '||' + deps[i+1] + '::'\n",
    "                            post_token_spacys.append(post_spacy + post_head_dep + post_head_spacy)\n",
    "                            \n",
    "                        for j, token in enumerate(before_tokens):\n",
    "                            if alignment_bef[j] == k:\n",
    "                                edit_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "                                print('D: ' + ''.join(pre_token_spacys) + '[' + edit_spacy + ']' + ''.join(post_token_spacys))\n",
    "\n",
    "\n",
    "            for head_i in head_is:\n",
    "                if after_tokens[i] in childs_texts[head_i]:  \n",
    "                    diff_token_h = line.split(' ')[alignment_aft[head_i]]\n",
    "                    if diff_token_h.endswith('+}'):\n",
    "                        edit_spacy = '(' + after_tokens[head_i] + ')' + lemmas[head_i] + '_' + tags[head_i]\n",
    "                        child_spacy = tags[i] + '_' + lemmas[i] + '('+ after_tokens[i] + ')'\n",
    "                        edit_child_dep = '::' + deps[i] + '||'\n",
    "                        if diff_token_h.startswith('[-'):\n",
    "                            for j, token in enumerate(before_tokens):\n",
    "                                if alignment_bef[j] == alignment_aft[head_i]:\n",
    "                                    edit_bef_spacy = '(' + before_tokens[j] + ')' + lemmas_bef[j] + '_' + tags_bef[j]\n",
    "                                    # return 'replace; child'\n",
    "                                    print('RepC: ' + edit_spacy + '[' + edit_bef_spacy + ']' + edit_child_dep + child_spacy)\n",
    "                        else:\n",
    "                            # return 'insert; child'\n",
    "                            print('InsC: ' + edit_spacy + edit_child_dep + child_spacy)\n",
    "        print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
