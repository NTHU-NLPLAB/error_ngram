{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = '''\n",
    "Hello [-.//PU-]{+!//PU+} {+I　'm//MW+} [-Fine//C-]{+fine//C+} {+,//PU+} thanks {+.//PU+} [-and//C-]{+And//C+} you ?\n",
    "Line up the bottles in rows of 4 , then 3 , then 2 , then 1 {+.//PU+} Get a frisbee per [-each//D-] player and allow to take two shots on each turn .\n",
    "Each pin {+knocked　down//MW+} is one point .\n",
    "{+For　a//MW+} [-Strike//C-]{+strike//C+} {+,//PU+} {+the　player//MW+} gets to take two more shots and [-add//AG-]{+adds//AG+} all the points together .\n",
    "The player with the most points is {+the//AR+} winner .\n",
    "Honey and ginger : [-it//PS-]{+they//PS+} [-is//AG-]{+are//AG+} [-natual//SP-]{+natural//SP+} food and good for sore throats [-,　no//PU-]{+,　no//PU+} side - effects , take a [-aspoonful//SP-]{+spoonful//SP+} anytime when you need .\n",
    "Garlic and Echinacea tea : drink it when {+you//PS+} have infection [-,　it//PU-]{+,　it//PU+} is simple but {+an//AR+} excellent antibiotic .\n",
    "Hot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains [-,　it//PU-]{+,　it//PU+} is {+a//AR+} fast and effective way to relieve aches and pains .\n",
    "[-everyone//C-]{+Everyone//C+} may use it in {+his　or　her//PS+} daily life .\n",
    "First , John asked Isabella not to marry him and [-giving　a　chance//XC-]{+her　to　give　him　the　chance//XC+} to prove himself to have {+the//AR+} ability to make [-the//AR-]{+a//AR+} happy life for her .\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 把標點符號edit token，變成after\n",
    "# 2. 簡化修改標記:  {+word+}, [-word-], [-word>>word+}\n",
    "# 3. 再斷句一次\n",
    "# 4. 把一句多錯誤，變成多句個含一個錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import fileinput\n",
    "import re\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def simple_tag(tags):\n",
    "    if tags['d'] and tags['i']:    # d >> i\n",
    "        return '[-{d}>>{i}+}}'.format(d=tags['d'], i=tags['i'])\n",
    "    elif tags['d']:\n",
    "        return '[-{d}-]'.format(d=tags['d'])\n",
    "    elif tags['i']:\n",
    "        return '{{+{i}+}}'.format(i=tags['i'])\n",
    "    else:\n",
    "        print(\"Should not be here in simple_tag()\")\n",
    "\n",
    "re_tag = r'(\\[-(?P<d>.+)//(?P<d_tag>.+)-\\])?({\\+(?P<i>.+)//(?P<i_tag>.+)\\+})?'\n",
    "def correct_punc(line):\n",
    "    new_line = []\n",
    "    for token in line.split(' '):\n",
    "        tags = re.match(re_tag, token).groupdict()\n",
    "        if not tags['d_tag'] and not tags['i_tag']:  # no edit, 原字\n",
    "            new_line.append(token)\n",
    "        elif tags['i_tag'] == 'PU':                  # PU 錯誤類型不管，因此遇到 PU 則改成正確句子，只管被新增的符號\n",
    "            for item in tags['i'].split():           # TODO: 照原本寫法，不確定 split 用意\n",
    "                new_line.append(item)\n",
    "        elif tags['d_tag'] != 'PU':                  # error type not 'PU'\n",
    "            new_line.append(simple_tag(tags))   \n",
    "    return' '.join(new_line)\n",
    "\n",
    "def restore_line_break(text):\n",
    "    return text.replace('<br/>', '\\n').replace('<br>', '\\n').replace('<br />', '\\n')\n",
    "\n",
    "def restore_xmlescape(text):\n",
    "    while '&amp;' in text:\n",
    "        text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&quote;', '\"')\n",
    "    text = text.replace('&quot;', '\"')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text\n",
    "\n",
    "def mask_edits(text):\n",
    "    edits, tokens = [], []\n",
    "    for token in text.split(' '):\n",
    "        if token.startswith('{+') or token.startswith('[-'):\n",
    "            masked_token = \"{{{0}}}\".format(len(edits))\n",
    "            tokens.append(masked_token)\n",
    "            edits.append(token)\n",
    "        else:\n",
    "            tokens.append(token.replace('{', '{{').replace('}', '}}'))\n",
    "    return ' '.join(tokens), edits\n",
    "\n",
    "\n",
    "def tokenize_doc(text):\n",
    "    text = restore_line_break(text)\n",
    "    text = restore_xmlescape(text)\n",
    "\n",
    "    # mask edit tokens first to prevent being segmented\n",
    "    # I have {+a+} pen. => I have {0} pen.\n",
    "    text_masked, edits = mask_edits(text)\n",
    "\n",
    "    for line in text_masked.splitlines():\n",
    "        for sent in sent_tokenize(line.strip()):\n",
    "            # restore masked edit tokens and return\n",
    "            yield sent.format(*edits) \n",
    "\n",
    "def to_after(tokens):\n",
    "    def to_after_token(token):\n",
    "        token = token.replace('\\u3000', ' ')\n",
    "        if token.endswith('-]'):\n",
    "            return None\n",
    "        elif token.endswith('+}'):\n",
    "            return token[token.rfind('>>')+2:-2]  if token.startswith('[-') else token[2:-2]  \n",
    "        else:\n",
    "            return token\n",
    "    return ' '.join(token for token in map(to_after_token, tokens) if token)\n",
    "\n",
    "new_data = []\n",
    "if __name__ == '__main__':\n",
    "    for line in test_data.split('\\n'): # fileinput.input():\n",
    "        simple_line = correct_punc(line.strip()) # remove PU\n",
    "        for sent in tokenize_doc(simple_line):\n",
    "            # after_sent = to_after(sent.split(' ')) # correct sentence\n",
    "            tokens = sent.split(' ')\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith('[-') or token.startswith('{+'):\n",
    "                    new_sent = to_after(tokens[:i]) + ' ' + token + ' ' + to_after(tokens[i+1:])\n",
    "                    new_data.append(new_sent.strip())\n",
    "\n",
    "\n",
    "# pprint(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "(Insert) \t->\tI 'm\n",
      "Sent:\t{+I　'm+} fine , thanks .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\t'm\t'm\tVBP\n",
      "Target:\tI\t-PRON-\tPRP\tnsubj\n",
      "\n",
      "Head:\t'm\t'm\tVBP\n",
      "Target:\t'm\t'm\tVBP\tROOT\n",
      "Child:\tI\t-PRON-\tPRP\tnsubj\n",
      "Child:\tfine\tfine\tJJ\tacomp\n",
      "Child:\t,\t,\t,\tpunct\n",
      "Child:\tthanks\tthank\tNNS\tnpadvmod\n",
      "Child:\t.\t.\t.\tpunct\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) Fine\t->\tfine\n",
      "Sent:\tI 'm [-Fine>>fine+} , thanks .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\t'm\t'm\tVBP\n",
      "Target:\tfine\tfine\tJJ\tacomp\n",
      "\n",
      "Delete:\tFine\tfine\tJJ\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) and\t->\tAnd\n",
      "Sent:\t[-and>>And+} you ?\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tyou\t-PRON-\tPRP\n",
      "Target:\tAnd\tand\tCC\tcc\n",
      "\n",
      "Delete:\tand\tand\tCC\n",
      "\n",
      "\n",
      "====================================\n",
      "(Delete) each\t->\t\n",
      "Sent:\tGet a frisbee per [-each-] player and allow to take two shots on each turn .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tfrisbee\tfrisbee\tNN\n",
      "Target:\tper\tper\tIN\tprep\n",
      "Child:\tplayer\tplayer\tNN\tpobj\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tper\tper\tIN\n",
      "Target:\tplayer\tplayer\tNN\tpobj\n",
      "\n",
      "Delete:\teach\teach\tDT\n",
      "Delete:\teach\teach\tDT\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tknocked down\n",
      "Sent:\tEach pin {+knocked　down+} is one point .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tis\tbe\tVBZ\n",
      "Target:\tknocked\tknock\tVBD\tcsubj\n",
      "Child:\tpin\tpin\tNN\tnsubj\n",
      "Child:\tdown\tdown\tRP\tprt\n",
      "\n",
      "Head:\tknocked\tknock\tVBD\n",
      "Target:\tdown\tdown\tRP\tprt\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tFor a\n",
      "Sent:\t{+For　a+} strike , the player gets to take two more shots and adds all the points together .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tgets\tget\tVBZ\n",
      "Target:\tFor\tfor\tIN\tprep\n",
      "Child:\tstrike\tstrike\tNN\tpobj\n",
      "\n",
      "Head:\tstrike\tstrike\tNN\n",
      "Target:\ta\ta\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) Strike\t->\tstrike\n",
      "Sent:\tFor a [-Strike>>strike+} , the player gets to take two more shots and adds all the points together .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tFor\tfor\tIN\n",
      "Target:\tstrike\tstrike\tNN\tpobj\n",
      "Child:\ta\ta\tDT\tdet\n",
      "\n",
      "Delete:\tStrike\tstrike\tNN\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tthe player\n",
      "Sent:\tFor a strike , {+the　player+} gets to take two more shots and adds all the points together .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tplayer\tplayer\tNN\n",
      "Target:\tthe\tthe\tDT\tdet\n",
      "\n",
      "Head:\tgets\tget\tVBZ\n",
      "Target:\tplayer\tplayer\tNN\tnsubj\n",
      "Child:\tthe\tthe\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) add\t->\tadds\n",
      "Sent:\tFor a strike , the player gets to take two more shots and [-add>>adds+} all the points together .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tgets\tget\tVBZ\n",
      "Target:\tadds\tadd\tVBZ\tconj\n",
      "Child:\tpoints\tpoint\tNNS\tdobj\n",
      "Child:\ttogether\ttogether\tRB\tadvmod\n",
      "\n",
      "Delete:\tadd\tadd\tVB\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tthe\n",
      "Sent:\tThe player with the most points is {+the+} winner .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\twinner\twinner\tNN\n",
      "Target:\tthe\tthe\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) it\t->\tthey\n",
      "Sent:\tHoney and ginger : [-it>>they+} are natural food and good for sore throats , no side - effects , take a spoonful anytime when you need .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tare\tbe\tVBP\n",
      "Target:\tthey\t-PRON-\tPRP\tnsubj\n",
      "\n",
      "Delete:\tit\t-PRON-\tPRP\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) is\t->\tare\n",
      "Sent:\tHoney and ginger : they [-is>>are+} natural food and good for sore throats , no side - effects , take a spoonful anytime when you need .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tare\tbe\tVBP\n",
      "Target:\tare\tbe\tVBP\tROOT\n",
      "Child:\tHoney\thoney\tNNP\tdep\n",
      "Child:\t:\t:\t:\tpunct\n",
      "Child:\tthey\t-PRON-\tPRP\tnsubj\n",
      "Child:\tfood\tfood\tNN\tattr\n",
      "Child:\t,\t,\t,\tpunct\n",
      "Child:\ttake\ttake\tVB\tconj\n",
      "Child:\t.\t.\t.\tpunct\n",
      "\n",
      "Delete:\tis\tbe\tVBZ\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) natual\t->\tnatural\n",
      "Sent:\tHoney and ginger : they are [-natual>>natural+} food and good for sore throats , no side - effects , take a spoonful anytime when you need .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tfood\tfood\tNN\n",
      "Target:\tnatural\tnatural\tJJ\tamod\n",
      "\n",
      "Delete:\tnatual\tnatual\tJJ\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) aspoonful\t->\tspoonful\n",
      "Sent:\tHoney and ginger : they are natural food and good for sore throats , no side - effects , take a [-aspoonful>>spoonful+} anytime when you need .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tanytime\tanytime\tNN\n",
      "Target:\tspoonful\tspoonful\tJJ\tamod\n",
      "\n",
      "Delete:\taspoonful\taspoonful\tJJ\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tyou\n",
      "Sent:\tGarlic and Echinacea tea : drink it when {+you+} have infection , it is simple but an excellent antibiotic .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\thave\thave\tVBP\n",
      "Target:\tyou\t-PRON-\tPRP\tnsubj\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tan\n",
      "Sent:\tGarlic and Echinacea tea : drink it when you have infection , it is simple but {+an+} excellent antibiotic .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tantibiotic\tantibiotic\tNN\n",
      "Target:\tan\tan\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\ta\n",
      "Sent:\tHot mixture of vinegar , olive oil and eucalyptus : place it on aches and pains , it is {+a+} fast and effective way to relieve aches and pains .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tway\tway\tNN\n",
      "Target:\ta\ta\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) everyone\t->\tEveryone\n",
      "Sent:\t[-everyone>>Everyone+} may use it in his or her daily life .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tuse\tuse\tVB\n",
      "Target:\tEveryone\teveryone\tNN\tnsubj\n",
      "\n",
      "Delete:\teveryone\teveryone\tNN\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\this or her\n",
      "Sent:\tEveryone may use it in {+his　or　her+} daily life .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tlife\tlife\tNN\n",
      "Target:\this\t-PRON-\tPRP$\tposs\n",
      "Child:\tor\tor\tCC\tcc\n",
      "\n",
      "Head:\this\t-PRON-\tPRP$\n",
      "Target:\tor\tor\tCC\tcc\n",
      "\n",
      "Head:\tlife\tlife\tNN\n",
      "Target:\ther\t-PRON-\tPRP$\tposs\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) giving a chance\t->\ther to give him the chance\n",
      "Sent:\tFirst , John asked Isabella not to marry him and [-giving　a　chance>>her　to　give　him　the　chance+} to prove himself to have the ability to make a happy life for her .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\thim\t-PRON-\tPRP\n",
      "Target:\ther\t-PRON-\tPRP\tconj\n",
      "\n",
      "Head:\tgive\tgive\tVB\n",
      "Target:\tto\tto\tTO\taux\n",
      "\n",
      "Head:\tmarry\tmarry\tVB\n",
      "Target:\tgive\tgive\tVB\tadvcl\n",
      "Child:\tto\tto\tTO\taux\n",
      "Child:\thim\t-PRON-\tPRP\tdative\n",
      "Child:\tchance\tchance\tNN\tdobj\n",
      "\n",
      "Head:\tgive\tgive\tVB\n",
      "Target:\thim\t-PRON-\tPRP\tdative\n",
      "\n",
      "Head:\tchance\tchance\tNN\n",
      "Target:\tthe\tthe\tDT\tdet\n",
      "\n",
      "Head:\tgive\tgive\tVB\n",
      "Target:\tchance\tchance\tNN\tdobj\n",
      "Child:\tthe\tthe\tDT\tdet\n",
      "Child:\tprove\tprove\tVB\tacl\n",
      "\n",
      "Delete:\tgiving\tgive\tVBG\n",
      "Delete:\ta\ta\tDT\n",
      "Delete:\tchance\tchance\tNN\n",
      "\n",
      "\n",
      "====================================\n",
      "(Insert) \t->\tthe\n",
      "Sent:\tFirst , John asked Isabella not to marry him and her to give him the chance to prove himself to have {+the+} ability to make a happy life for her .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tability\tability\tNN\n",
      "Target:\tthe\tthe\tDT\tdet\n",
      "\n",
      "\n",
      "\n",
      "====================================\n",
      "(Replace) the\t->\ta\n",
      "Sent:\tFirst , John asked Isabella not to marry him and her to give him the chance to prove himself to have the ability to make [-the>>a+} happy life for her .\n",
      "\n",
      "\tToken\tLemma\tTag\tDep(to head)\n",
      "Head:\tlife\tlife\tNN\n",
      "Target:\ta\ta\tDT\tdet\n",
      "\n",
      "Delete:\tthe\tthe\tDT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用來抓 edit word\n",
    "re_words = r'(\\[-(?P<d>.+)-\\]|{\\+(?P<i>.+)\\+}|\\[-(?P<rd>.+)>>(?P<ri>.+)\\+})?'\n",
    "def correct(origin_tokens):\n",
    "    correct_tokens, pairs = [], []\n",
    "    for ot in origin_tokens:\n",
    "        ot = ot.replace('\\u3000', ' ')\n",
    "        words = re.match(re_words, ot).groupdict()\n",
    "        if words['rd'] and words['ri']:\n",
    "            pairs.append(('Replace', words['rd'], words['ri'], len(correct_tokens))) # 最後一欄位是對應 correct_tokens 用的\n",
    "            for ri in words['ri'].split():\n",
    "                correct_tokens.append(ri)\n",
    "        elif words['i']:\n",
    "            pairs.append(('Insert', \"\", words['i'], len(correct_tokens)))\n",
    "            for i in words['i'].split():\n",
    "                correct_tokens.append(i)\n",
    "        elif words['d']:\n",
    "            pairs.append(('Delete', words['d'], \"\", len(correct_tokens)))\n",
    "        else:\n",
    "            correct_tokens.append(ot)\n",
    "            \n",
    "    return correct_tokens, pairs\n",
    "\n",
    "    \n",
    "def format_edit(edit):\n",
    "    line, line_edits = edit\n",
    "    edit_type, origin_token, new_token, correct_token = line_edits[0]\n",
    "    \n",
    "    delete = \"\"\n",
    "    template = \"(\"+ edit_type +\") \"+ origin_token +\"\\t->\\t\"+ new_token +\"\\nSent:\\t\"+ line +\"\\n\\n\"\n",
    "    \n",
    "    for e in line_edits:\n",
    "        edit_type, origin_token, new_token, correct_token = e\n",
    "        \n",
    "        template += \"\\tToken\\tLemma\\tTag\\tDep(to head)\\n\"\n",
    "        for t in correct_token: # Insert or Replace\n",
    "            template += \"Head:\\t\"+ t.head.text +\"\\t\"+ t.head.lemma_ +\"\\t\" + t.head.tag_ +\"\\n\"\n",
    "            template += \"Target:\\t\"+ t.text +\"\\t\"+ t.lemma_ +\"\\t\"+ t.tag_ +\"\\t\" + t.dep_ +\"\\n\"\n",
    "\n",
    "            for child in t.children:\n",
    "                template += \"Child:\\t\"+ child.text +\"\\t\"+ child.lemma_ +\"\\t\"+ child.tag_ +\"\\t\" + child.dep_ +\"\\n\"\n",
    "            template += \"\\n\"\n",
    "            \n",
    "        if origin_token:\n",
    "            origin_token = nlp(origin_token)\n",
    "            for ot in origin_token:\n",
    "                delete += \"Delete:\\t\"+ ot.text +\"\\t\"+ ot.lemma_ +\"\\t\"+ ot.tag_ +\"\\n\"\n",
    "                \n",
    "    return template + delete\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    all_edits = []\n",
    "    for line in new_data: # fileinput.input():\n",
    "        origin_tokens = line.strip().split(' ')\n",
    "        correct_tokens, edit_pairs = correct(origin_tokens) # get edit pairs\n",
    "        if not correct_tokens or not edit_pairs: continue # skip no edit or empty string\n",
    "            \n",
    "        correct_tokens = nlp(' '.join(correct_tokens))\n",
    "        \n",
    "        line_edits = []\n",
    "        for pair in edit_pairs: # 照理只有一個 pair\n",
    "            edit_type, origin_token, new_token, index = pair\n",
    "            \n",
    "            if edit_type == \"Delete\":\n",
    "                if index < len(correct_tokens):\n",
    "                    line_edits.append((edit_type, origin_token, new_token, correct_tokens[index-1:index]))\n",
    "                if index > 0:\n",
    "                    line_edits.append((edit_type, origin_token, new_token, correct_tokens[index:index+1]))\n",
    "            else:\n",
    "                line_edits.append((edit_type, origin_token, new_token, correct_tokens[index:index+len(new_token.split())]))\n",
    "                \n",
    "        all_edits.append((line, line_edits))\n",
    "\n",
    "    for edit in all_edits:\n",
    "        print(\"\\n====================================\")\n",
    "        print(format_edit(edit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7c23b917a865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'An'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d8506bebea25>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# All tokens 'own' a subsequent space character in this tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mspaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.push_back\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a=nlp('')\n",
    "a[:1][0].tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reverseBits(n):\n",
    "    n = list(format(n, '032b'))\n",
    "    for i in range(16):\n",
    "        n[i], n[31-i] = n[31-i], n[i]\n",
    "    print(n)\n",
    "    return int(''.join(n), 2)\n",
    "    reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2147483648"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseBits(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
